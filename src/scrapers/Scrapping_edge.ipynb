{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc092e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f3d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "email_value = os.getenv('email')  # tu email\n",
    "pass_value = os.getenv('pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffa083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FacebookScraper:\n",
    "    def __init__(self, email, password):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "        \n",
    "    def initialize_driver(self):\n",
    "        \"\"\"Initialize the Edge webdriver with custom options\"\"\"\n",
    "        options = webdriver.EdgeOptions()\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "        \n",
    "        self.driver = webdriver.Edge(options=options)\n",
    "        self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        \n",
    "    def simulate_human_typing(self, element, text):\n",
    "        \"\"\"Simulate human-like typing patterns\"\"\"\n",
    "        for char in text:\n",
    "            element.send_keys(char)\n",
    "            time.sleep(random.uniform(0.1, 0.3))\n",
    "            if random.random() < 0.1:\n",
    "                time.sleep(random.uniform(0.3, 0.7))\n",
    "                \n",
    "    def login(self):\n",
    "        \"\"\"Login to Facebook\"\"\"\n",
    "        self.driver.get(\"https://www.facebook.com/login\")\n",
    "        \n",
    "        # Enter email\n",
    "        email_input = WebDriverWait(self.driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"email\"))\n",
    "        )\n",
    "        self.simulate_human_typing(email_input, self.email)\n",
    "        \n",
    "        # Enter password\n",
    "        password_input = WebDriverWait(self.driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"pass\"))\n",
    "        )\n",
    "        self.simulate_human_typing(password_input, self.password)\n",
    "        \n",
    "        # Click login button\n",
    "        login_button = self.driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "        ActionChains(self.driver)\\\n",
    "            .move_to_element(login_button)\\\n",
    "            .pause(random.uniform(0.2, 0.4))\\\n",
    "            .click()\\\n",
    "            .perform()\n",
    "            \n",
    "        time.sleep(15)\n",
    "        \n",
    "    def navigate_to_profile(self, profile_url):\n",
    "        \"\"\"Navigate to a specific Facebook profile\"\"\"\n",
    "        self.driver.get(profile_url)\n",
    "        time.sleep(4)\n",
    "        \n",
    "    def slow_scroll(self, step=500):\n",
    "        \"\"\"Scroll the page slowly\"\"\"\n",
    "        self.driver.execute_script(f\"window.scrollBy(0, {step});\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "    def extract_posts_with_bs(self):\n",
    "        \"\"\"Extract posts data using BeautifulSoup\"\"\"\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        posts_data = []\n",
    "        \n",
    "        posts = soup.find_all(\"div\", {\"class\": \"x1n2onr6 x1ja2u2z\"})\n",
    "        \n",
    "        for post in posts:\n",
    "            try:\n",
    "                message_elements = post.find_all(\"div\", {\"data-ad-preview\": \"message\"})\n",
    "                post_text = \" \".join([msg.get_text(strip=True) for msg in message_elements])\n",
    "                \n",
    "                likes_element = post.select_one(\"span.xt0b8zv.x1jx94hy.xrbpyxo.xl423tq > span > span\")\n",
    "                likes = likes_element.get_text(strip=True) if likes_element else None\n",
    "                \n",
    "                comments_element = post.select(\"div > div > span > div > div > div > span > span.html-span \")\n",
    "                comments = comments_element[0].text if comments_element else None\n",
    "                \n",
    "                \n",
    "                shares_element =post.select(\"div > div > span > div > div > div > span > span.html-span \")\n",
    "                shares = shares_element[1].text if shares_element else None\n",
    "\n",
    "                timeelement=post.select_one(\"div.xu06os2.x1ok221b > span > div > span > span > a > span\")\n",
    "                post_time= timeelement.get_text(strip=True) if timeelement else None\n",
    "\n",
    "                \n",
    "                posts_data.append({\n",
    "                    \"post_text\": post_text,\n",
    "                    \"likes\": likes,\n",
    "                    \"comments\": comments,\n",
    "                    \"shares\": shares,\n",
    "                    \"post_time\": post_time\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Error extracting post data:\", e)\n",
    "                \n",
    "        return posts_data\n",
    "        \n",
    "    def remove_duplicates(self, data_list):\n",
    "        \"\"\"Remove duplicate posts\"\"\"\n",
    "        seen = set()\n",
    "        unique_data = []\n",
    "        for data in data_list:\n",
    "            data_tuple = tuple(data.items())\n",
    "            if data_tuple not in seen:\n",
    "                seen.add(data_tuple)\n",
    "                unique_data.append(data)\n",
    "        return unique_data\n",
    "        \n",
    "    def scroll_to_load_posts(self, scroll_count=50):\n",
    "        \"\"\"Solo hacer scroll para cargar posts sin extraer datos\"\"\"\n",
    "        print(f\"Iniciando scroll para cargar posts...\")\n",
    "        \n",
    "        for i in range(scroll_count):\n",
    "            print(f\"Scroll {i + 1}/{scroll_count}\")\n",
    "            self.slow_scroll()\n",
    "            \n",
    "            # Pausa ocasional para simular comportamiento humano\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                print(\"Pausa para simular comportamiento humano...\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "                \n",
    "        print(\"Scroll completado. Los datos se capturar√°n con c.js\")\n",
    "\n",
    "    def print_posts(self, posts_data):\n",
    "        \"\"\"Print the scraped posts data\"\"\"\n",
    "        for idx, post in enumerate(posts_data, start=1):\n",
    "            print(f\"Post {idx}:\")\n",
    "            print(f\"Text: {post['post_text']}\")\n",
    "            print(f\"Likes: {post['likes']}\")\n",
    "            print(f\"Comments: {post['comments']}\")\n",
    "            print(f\"Shares: {post['shares']}\")\n",
    "            print(f\"Time Posted: {post['post_time']}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "    def close(self):\n",
    "        \"\"\"Close the browser\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the scraper\n",
    "\n",
    "    scraper = FacebookScraper(email=email_value, password=pass_value)\n",
    "    \n",
    "    try:\n",
    "        # Setup and login\n",
    "        scraper.initialize_driver()\n",
    "        scraper.login()\n",
    "        \n",
    "        # Navigate to Facebook group\n",
    "        scraper.navigate_to_profile(\"https://www.facebook.com/groups/973136421423238/\")\n",
    "        \n",
    "        # Solo hacer scroll para cargar posts (sin extraer datos)\n",
    "        scraper.scroll_to_load_posts(scroll_count=5000)\n",
    "        \n",
    "        print(\"Ahora ejecuta el script c.js en la consola del navegador para capturar los datos\")\n",
    "        \n",
    "        # Mantener el navegador abierto para usar c.js\n",
    "        input(\"Presiona Enter cuando hayas terminado de usar c.js...\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up\n",
    "        scraper.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
